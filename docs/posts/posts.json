[
  {
    "path": "posts/2024-10-10-best-rainfall-prediction-model-based-on-skill-score/",
    "title": "Best Rainfall Prediction Model Based on Skill Score",
    "description": "This content contains information and techniques in determining the best prediction model using the skill score method.",
    "author": [
      {
        "name": "Eggy Pandiangan",
        "url": {}
      }
    ],
    "date": "2024-10-10",
    "categories": [],
    "contents": "\r\n\r\nContents\r\n1. INTRODUCTION\r\n2. HOW IT IS EVALUATED?\r\n3. RAINFALL PREDICTION EVALUATION (FOR SEASON ONSET DETERMINATION)\r\n4. RAINFALL WEIGHTING\r\n5. SIMULATION OF SKILL SCORE METHOD\r\n\r\n1. INTRODUCTION\r\nDefinition and Goals of Climate Seasonal Forecast Assessment\r\n\r\nA Climate Seasonal Forecast Assessment refers to the systematic evaluation of seasonal climate forecasts, which typically predict weather patterns over several months. This process involves analyzing the accuracy, skill, and reliability of these forecasts to determine how closely they match the actual weather outcomes (observation). The primary goal of this assessment is to verify whether the forecasted seasonal conditions, such as temperature, precipitation, and wind patterns, align with the observed climate data for the given period.\r\n\r\n\r\nThe verification aspect plays a crucial role in this assessment, as it measures the forecast’s skill and consistency. Verification involves comparing the predicted values with real-world data, often using statistical techniques to determine the forecast’s accuracy. The results of the verification help meteorologists refine forecasting models, identify areas for improvement, and better understand the underlying climatic variables that influence seasonal patterns. Validation is the confirmation through testing and providing objective evidence that certain requirements for a specific purpose are met by a model. Verification, on the other hand, is the process of comparing model calculations (forecasts) with actual values (observations), which is generally equated with validation (Jolliffe and Stephenson 2011).\r\n\r\n\r\nThe type of verification adjusts to the type of forecasts, here are some examples of the types of forecasts and verification (WWRP 2017)\r\n\r\n\r\nNature of forecast:\r\n\r\n\r\nExample(s):\r\n\r\n\r\nVerification methods:\r\n\r\n\r\ndeterministic (non-probabilistic)\r\n\r\n\r\nquantitative precipitation forecast\r\n\r\n\r\nvisual, dichotomous, multi-category, continuous, spatial\r\n\r\n\r\nprobabilistic\r\n\r\n\r\nprobability of precipitation, ensemble forecast\r\n\r\n\r\nvisual, probabilistic, ensemble\r\n\r\n\r\nqualitative (worded)\r\n\r\n\r\n5-day outlook\r\n\r\n\r\nvisual, dichotomous, multi-category\r\n\r\n\r\nSpace-time domain:\r\n\r\n\r\n\r\n\r\n\r\n\r\ntime series\r\n\r\n\r\ndaily maximum temperature forecasts for a city\r\n\r\n\r\nvisual, dichotomous, multi-category, continuous, probabilistic\r\n\r\n\r\nspatial distribution\r\n\r\n\r\nmap of geopotential height, rainfall chart\r\n\r\n\r\nvisual, dichotomous, multi-category, continuous, probabilistic, spatial, ensemble\r\n\r\n\r\npooled space and time\r\n\r\n\r\nmonthly average global temperature anomaly\r\n\r\n\r\ndichotomous, multi-category, continuous, probabilistic, ensemble\r\n\r\n\r\nSpecificity of forecast:\r\n\r\n\r\n\r\n\r\n\r\n\r\ndichotomous (yes/no)\r\n\r\n\r\noccurrence of fog\r\n\r\n\r\nvisual, dichotomous, probabilistic, spatial, ensemble\r\n\r\n\r\nmulti-category\r\n\r\n\r\ncold, normal, or warm conditions\r\n\r\n\r\nvisual, multi-category, probabilistic, spatial, ensemble\r\n\r\n\r\ncontinuous\r\n\r\n\r\nmaximum temperature\r\n\r\n\r\nvisual, continuous, probabilistic, spatial, ensemble\r\n\r\n\r\nobject- or event-oriented\r\n\r\n\r\ntropical cyclone motion and intensity\r\n\r\n\r\nvisual, dichotomous, multi-category, continuous, probabilistic, spatial\r\n\r\n\r\nAllan Murphy (Murphy 1993), a pioneer in the field of prediction verification, distinguishes 3 types of ‘goodness’ of a prediction :\r\nConsistency - the degree to which the forecast corresponds to the forecaster’s best judgement about the situation, based upon his/her knowledge base.\r\nQuality - the degree to which the forecast corresponds to what actually happened (observation).\r\nValue - the degree to which the forecast helps a decision maker to realize some incremental economic and/or other benefit.\r\n\r\nIn addition, Murphy also mentioned that there are 8 aspects (attributes) that show the quality of a forecast, including:\r\n\r\nNo.\r\n\r\n\r\nAspects/Attributes\r\n\r\n\r\nDescription\r\n\r\n\r\nScores/metrics that can be used\r\n\r\n\r\n1\r\n\r\n\r\nBias\r\n\r\n\r\nDifference between predicted and observed means\r\n\r\n\r\nMean Error (ME), Mean Square Error (MSE), Scatter plot\r\n\r\n\r\n2\r\n\r\n\r\nAccuracy\r\n\r\n\r\nThe degree of agreement between prediction and observation\r\n\r\n\r\nContinuous Rank Probability Score (CRPS), Taylor Diagram\r\n\r\n\r\n3\r\n\r\n\r\nUncertainty\r\n\r\n\r\nThe diversity of observation values, the greater the uncertainty of the observation, the more difficult it is to predict\r\n\r\n\r\nVerification Rank Histogram (VRH)\r\n\r\n\r\n4\r\n\r\n\r\nSharpness\r\n\r\n\r\nThe ability of the forecast to predict extreme values. Sharpness is “only” possessed by predictions (not observations); even poor predictions still have the attribute of sharpness\r\n\r\n\r\nEnsemble Spread (SPRD), Spread Skill Relationship (SSR)\r\n\r\n\r\n5\r\n\r\n\r\nResolution\r\n\r\n\r\nThe ability of a forecast to represent how much the prediction differs from the probabilistic mean of the climatology of an event and whether the prediction system can predict it correctly; Typically used to measure the mean square of probabilistic prediction error\r\n\r\n\r\nBrier Score (BS), Relative Operating Characteristic (ROC)\r\n\r\n\r\n6\r\n\r\n\r\nDiscrimination\r\n\r\n\r\nThe forecast’s ability to clearly distinguish a situation that leads to the occurrence or non-occurrence of an event\r\n\r\n\r\nRelative Operating Characteristic (ROC)\r\n\r\n\r\n7\r\n\r\n\r\nReliability\r\n\r\n\r\nStatistical consistency between the probabilistic prediction of an event and the actual frequency of occurrence\r\n\r\n\r\nReliability Diagram (RD), Brier Score (BS)\r\n\r\n\r\n8\r\n\r\n\r\nSkill\r\n\r\n\r\nThe relative accuracy of a prediction model to a reference (climatological conditions) or an increase in prediction accuracy due to an improved prediction system; Reliability is used to measure the superiority of a prediction system based on a baseline of past observations\r\n\r\n\r\nSkill Score (SS): BSS, CRPSS, ROCSS\r\n\r\nExamples of Evaluations for Climate Seasonal Forecasts\r\n\r\nSome meteorological agencies in other countries have evaluated their predictions using several verification metrics, for example:\r\n\r\n\r\n\r\n\r\n\\(~\\)\r\n2. HOW IT IS EVALUATED?\r\nApproaches for Assessing Climate Seasonal Forecast\r\n\r\nThere are several approaches that are commonly used to assess seasonal climate forecasts (Wilks 2011; Jolliffe and Stephenson 2011; WWRP 2017), such as:\r\n\r\n\r\nNo.\r\n\r\n\r\nMetric\r\n\r\n\r\nFormulation\r\n\r\n\r\nDescription\r\n\r\n\r\n1\r\n\r\n\r\nMean Error (ME)\r\n\r\n\r\n\\(\\text{ME}=\\frac{\\sum_{i=1}^{n}\\left(f_i-o_i\\right)}{n}\\)\r\n\r\n\r\n\\(n=\\) Number of data pairs (forecast & observation); \\(f_i=\\) Forecast value; \\(o_i=\\) Observatioin value; Best Score \\(=0\\); Verification method = continuous;\r\n\r\n\r\n2\r\n\r\n\r\nMean Absolute Error (MAE)\r\n\r\n\r\n\\(\\text{MAE}=\\frac{\\sum_{i=1}^{n}\\left|f_i-o_i\\right|}{n}\\)\r\n\r\n\r\n\\(n=\\) Number of data pairs (forecast & observation); \\(f_i=\\) Forecast value; \\(o_i=\\) Observatioin value; Best Score \\(=0\\); Verification method = continuous;\r\n\r\n\r\n3\r\n\r\n\r\nRoot Mean Square Error (RMSE)\r\n\r\n\r\n\\(\\text{RMSE}=\\sqrt{\\frac{\\sum_{i=0}^{N - 1} (f_i - o_i)^2}{N}}\\)\r\n\r\n\r\n\\(N=\\) Number of data pairs (forecast & observation); \\(f_i=\\) Forecast value; \\(o_i=\\) Observatioin value; Best Score \\(=0\\); Verification method = continuous;\r\n\r\n\r\n4\r\n\r\n\r\nBoxplot\r\n\r\n\r\n\r\n\r\n\\(Q_1=(n+1)*0.25\\); \\(Q_2=(n+1)*0.5\\); \\(Q_3=(n+1)*0.75\\); \\(IQR=Q_3-Q_1\\); Best Score = Closest to Observation; Verification method = continuous, visual;\r\n\r\n\r\n5\r\n\r\n\r\nScatter plot\r\n\r\n\r\n\r\n\r\nBest Score = gather around the diagonal; Verification method = continuous, visual;\r\n\r\n\r\n6\r\n\r\n\r\nCorrelation Coefficient \\((r)\\)\r\n\r\n\r\n\\(r=\\frac{\\sum{(f_i-\\overline{f})(o_i-\\overline{o})}}{\\sqrt{\\sum{(f_i-\\overline{f})^2}}\\sqrt{\\sum{(o_i-\\overline{o})^2}}}\\)\r\n\r\n\r\n\\(f_i=\\) i-th forecast value; \\(\\overline{f}=\\) mean of forecast values; \\(o_i=\\) i-th observation value; \\(\\overline{f}=\\) mean of observation values; Best Score = 1; Verification method = continuous;\r\n\r\n\r\n7\r\n\r\n\r\nDichotomous Contingency Table\r\n\r\n\r\n\r\n\r\n\\(\\text{HR} = \\frac{a}{a + c} = \\hat{p}(\\hat{x} = 1 \\mid x = 1)\\); \\(\\text{PC or Accuracy} = \\frac{a + d}{n} = \\hat{p}\\left[(\\hat{x} = 1, x = 1) \\, \\text{or} \\, (\\hat{x} = 0, x = 0)\\right]\\);\\(\\text{FAR} = \\frac{b}{a + b} = \\hat{p}(x = 0 \\mid \\hat{x} = 1)\\); \\(\\text{CSI} = \\frac{a}{a + b + c}\\);ROC\r\n\r\n\r\n8\r\n\r\n\r\nMulti-Category Contingency Table\r\n\r\n\r\n\r\n\r\n\\(\\text{Accuracy} = \\frac{1}{N} \\sum_{i=1}^{K}n(F_i, O_i)\\); \\(\\text{HSS} = \\frac{\\frac{1}{N} \\sum_{i=1}^{K} n(F_i, O_i)-\\frac{1}{N^2} \\sum_{i=1}^{K} N(F_i) N(O_i)}{1 - \\frac{1}{N^2} \\sum_{i=1}^{K} N(F_i) N(O_i)}\\);   Best Score Accuracy=1; Best Score HSS=1\r\n\r\nSkill Score Method\r\n\r\nPrediction of the onset of the Indonesian season by BMKG for both the wet and dry seasons is based on rainfall values obtained from the output of several models. These models consist of dynamic and statistical models that have their own advantages and disadvantages. The number of models used will certainly cause more uncertainty in the prediction of season onset. To overcome this, several things can be done :\r\n\r\nUse the ensemble mean of all rainfall models and then determine the onset of the season (at ZOM area);\r\nUsing the season onset mode from all models at the ZOM area;\r\nUsing the best model output in the ZOM area, and the best model can be found with:\r\nModel evaluation based on historical predictions and observations of season onset\r\nModel evaluation based on historical prediction and observation of rainfall\r\n\r\n\r\nSome of the metrics previously described can be used to evaluate the rainfall output of models used in seasonal prediction. Here we will use an evaluation metric called the Taylor Skill Score (TSS)(Xiaoli Yang and Sheffield 2020) or commonly called Skill Score (SS), whose basis is the Taylor Diagram. The Taylor Diagram has 3 output results (Correlation Coefficient, RMSE, and \\({NSD}_m\\)) drawn in one graph. If referring to each of the Taylor Diagram outputs, it will certainly be difficult to determine the best model. TSS combines the 3 outputs that produce a value that states the skill of the rainfall prediction model being evaluated.\r\n\r\n\\[\\begin{equation}\r\n\\tag{1}\r\nSS = \\frac{4(1 + CC)^4}{\\left( \\text{NSD}_m + \\frac{1}{\\text{NSD}_m} \\right)^2 (1 + CC_0)^4}\r\n\\end{equation}\\]\r\n\\[\\begin{equation}\r\n\\tag{2}\r\n\\text{NSD}_m = \\frac{\\sigma_{\\text{mod}}}{\\sigma_{\\text{obs}}}\r\n\\end{equation}\\]\r\nWhere:\r\n\\[\r\n\\text{NSD}_m = \\text{Normalized standard deviation of the simulation (model)}\r\n\\]\r\n\\[\r\n\\sigma_{\\text{mod}} = \\text{Standard deviation of the simulation (model)}\r\n\\]\r\n\\[\r\n\\sigma_{\\text{obs}} = \\text{Standard deviation of the observation}\r\n\\]\r\n\\[\r\nm = \\text{Model value simulation}\r\n\\]\r\n\\[\r\n\\text{CC}_0 = \\text{Maximum correlation coefficient}\r\n\\]\r\n\\[\r\n\\text{CC} = \\text{Correlation coefficient between the simulated (model) and observed data}\r\n\\]\r\n\r\nThe closer SS is to 1, the better the ability of the individual model to represent the observations\r\n\r\n\\(~\\)\r\n3. RAINFALL PREDICTION EVALUATION (FOR SEASON ONSET DETERMINATION)\r\nAssessment at ZOM 9120\r\n\r\nHere is an example of rainfall forecast data from several models (Initial January 2011) and observations, for 21 dekad days (ZOM Aceh_01):\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nCODE TO CREATE A TAYLOR DIAGRAM\r\n\r\n\r\n\r\nlibrary(openair)\r\nlibrary(Metrics)\r\nlibrary(tidyr)\r\n#dt <- read.csv('path/to/your/data.csv)\r\ndt1 <- pivot_longer(data = dt, cols = colnames(dt[!colnames(dt) %in% c('MODEL')]), names_to = 'DAS', values_to = 'CH')\r\ndt1$MODEL <- factor(dt1$MODEL, levels = unique(dt1$MODEL))\r\ndt1$DAS <- factor(dt1$DAS, levels = unique(dt1$DAS))\r\nt_dt <- as.data.frame(t(dt)); nms <- t_dt[1,]; t_dt <- t_dt[2:nrow(t_dt),]; colnames(t_dt) <- nms; t_dt$DAS <- rownames(t_dt)\r\nt_dt[!colnames(t_dt) %in% c('DAS')] <- as.data.frame(sapply(t_dt[!colnames(t_dt) %in% c('DAS')], as.numeric))\r\nt_dt2 <- t_dt\r\nt_dt1 <- as.data.frame(pivot_longer(data = t_dt, cols = colnames(t_dt[!colnames(t_dt) %in% c('DAS','OBS')]), names_to = 'MODEL', values_to = 'CH'))\r\nt_dt1[!colnames(t_dt1) %in% c('MODEL','DAS')] <- sapply(t_dt1[!colnames(t_dt1) %in% c('MODEL','DAS')], as.numeric)\r\nt_dt1$MODEL <- factor(t_dt1$MODEL, levels = unique(t_dt1$MODEL))\r\np2 <- TaylorDiagram(t_dt1, obs = \"OBS\", mod = \"CH\", group = \"MODEL\", normalise = T, cols = c('cornflowerblue','blue','yellow2','deeppink','deeppink4','orange1','orange4'),\r\n                    text.obs = 'Observasi', key = T, main=paste0('Taylor Diagram'))\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nECMWF_RAW\r\n\r\n\r\nECMWF_COR\r\n\r\n\r\nMME1\r\n\r\n\r\nCFSv2_RAW\r\n\r\n\r\nCFSv2_COR\r\n\r\n\r\nARIMA\r\n\r\n\r\nWARIMA\r\n\r\n\r\nRMSE\r\n\r\n\r\n42.9393894\r\n\r\n\r\n32.8570424\r\n\r\n\r\n38.5247122\r\n\r\n\r\n46.4539831\r\n\r\n\r\n31.4249754\r\n\r\n\r\n43.5126742\r\n\r\n\r\n42.0122118\r\n\r\n\r\nCC\r\n\r\n\r\n0.6972082\r\n\r\n\r\n0.7312276\r\n\r\n\r\n0.5420013\r\n\r\n\r\n0.4109492\r\n\r\n\r\n0.7422360\r\n\r\n\r\n0.3165167\r\n\r\n\r\n0.4026214\r\n\r\n\r\nNSDm\r\n\r\n\r\n0.6112191\r\n\r\n\r\n0.5841298\r\n\r\n\r\n0.6217545\r\n\r\n\r\n0.6297243\r\n\r\n\r\n0.7395452\r\n\r\n\r\n0.3083050\r\n\r\n\r\n0.3535605\r\n\r\n\r\nSS\r\n\r\n\r\n0.4107342\r\n\r\n\r\n0.4259743\r\n\r\n\r\n0.2842024\r\n\r\n\r\n0.2014521\r\n\r\n\r\n0.5264507\r\n\r\n\r\n0.0595302\r\n\r\n\r\n0.0955697\r\n\r\n\r\n\r\nBased on the above example, the best model in ZOM ACEH_01 for the January 2011 prediction initial is the CFSv2_COR Model. SS calculations should be carried out throughout the availability of data (in this example from 2011 - 2020) and the entire initial (January - December). So that the SS will be obtained for each month (January - December) for 10 years (2011 - 2020). Based on all the SS obtained, the average SS for the 10 years can be calculated and the best model with the highest SS is obtained.\r\n\r\n\r\nBEST MODEL FOR EACH INITIAL BASED ON AVERAGE SS 2011-2020\r\n\r\n\r\n\r\nSelect Sheet:\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nUsing the Skill Score method to Interpret the Evaluation\r\nBoxplot dari SS.\r\n\r\nLorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry’s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\r\n\r\n4. RAINFALL WEIGHTING\r\nSeasonal Rainfall Prediction Weighting based on Skill Score\r\n\r\nLorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry’s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\r\n\r\nSeasonal Rainfall Prediction ZOM 9120 using Skill Score-based weighting\r\n\r\nLorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry’s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\r\n\r\n5. SIMULATION OF SKILL SCORE METHOD\r\n\r\nLorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry’s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\r\n\r\n\r\n\r\n\r\nVar1\r\nFreq\r\nECA\r\n3\r\nEconomics\r\n2\r\nGeoSciences\r\n1\r\nLAW\r\n6\r\nPPLS\r\n5\r\nSLLC\r\n1\r\nSSPS\r\n2\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nJolliffe, I. T., and D. B. Stephenson. 2011. Forecast Verification: A Practitioner’s Guide in Atmospheric Science. Wiley. https://books.google.co.id/books?id=sgwIEAAAQBAJ.\r\n\r\n\r\nMurphy, Allan H. 1993. “What Is a Good Forecast? An Essay on the Nature of Goodness in Weather Forecasting.” Weather and Forecasting 8 (2): 281–93. https://doi.org/10.1175/1520-0434(1993)008<0281:WIAGFA>2.0.CO;2.\r\n\r\n\r\nWilks, Daniel S. 2011. Statistical Methods in the Atmospheric Sciences. Amsterdam; Boston: Elsevier Academic Press. https://www.amazon.com/Statistical-Atmospheric-Sciences-International-Geophysics/dp/0123850223/ref=pd_bxgy_14_img_3?_encoding=UTF8&psc=1&refRID=ESPQQ0R2PB1TP1VJSGCZ.\r\n\r\n\r\nWWRP. 2017. “WWRP/WGNE Joint Working Group on Forecast Verification Research.” https://www.cawcr.gov.au/projects/verification.\r\n\r\n\r\nXiaoli Yang, Yuqian Wang, Xiaohan Yu, and Justin Sheffield. 2020. “The Optimal Multimodel Ensemble of Bias-Corrected CMIP5 Climate Models over China.” Journal of Hydrometeorology 21 (4): 845–63. https://doi.org/10.1175/JHM-D-19-0141.1.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-10-10-best-rainfall-prediction-model-based-on-skill-score/zom_type.png",
    "last_modified": "2024-10-29T15:08:20+07:00",
    "input_file": "best-rainfall-prediction-model-based-on-skill-score.knit.md"
  }
]
